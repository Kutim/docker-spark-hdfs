version: "3"
services:
  master:
    image: rafabsb/docker-spark-hdfs
    command: bin/spark-class org.apache.spark.deploy.master.Master -h master
    env_file:
      - ./project.env
    command: /usr/hadoop-2.7.3/etc/hadoop/run.sh
    hostname: master
    networks:
      - default_apps
    environment:
      HADOOP_TYPE: namenode
      MASTER: spark://master:7077
      SPARK_CONF_DIR: /conf
      SPARK_PUBLIC_DNS: localhost
    expose:
      - 7001
      - 7002
      - 7003
      - 7004
      - 7005
      - 7006
      - 7077
      - 6066
      - 8020
    ports:
      - 4040:4040
      - 6066:6066
      - 7077:7077
      - 8080:8080
      - 50070:50070
      - 50090:50090
      - 8020:8020
    volumes:
      - ./volumes/spark/conf/master:/conf
      - ./volumes/spark/data:/tmp/data
      - ./volumes/spark/dependencies:/tmp/dependencies

  worker1:
    image: rafabsb/docker-spark-hdfs
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    env_file:
      - ./project.env
    command: /usr/hadoop-2.7.3/etc/hadoop/run.sh
    hostname: worker1
    networks:
      - default_apps
    environment:
      HADOOP_TYPE: datanode
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 4
      SPARK_WORKER_MEMORY: 4g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
      SPARK_WORKER_DIR: /tmp
    links:
      - master
    expose:
      - 7012
      - 7013
      - 7014
      - 7015
      - 7016
      - 8881
    ports:
      - "8081:8081"
      - "4040"
    volumes:
      - ./volumes/spark/conf/worker:/conf
      - ./volumes/spark/data:/tmp/data
      - ./volumes/spark/dependencies:/tmp/dependencies

    depends_on:
      - master

  worker2:
    image: rafabsb/docker-spark-hdfs
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    env_file:
      - ./project.env
    command: /usr/hadoop-2.7.3/etc/hadoop/run.sh
    hostname: worker2
    networks:
      - default_apps
    environment:
      HADOOP_TYPE: datanode
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 4
      SPARK_WORKER_MEMORY: 4g
      SPARK_WORKER_PORT: 8882
      SPARK_WORKER_WEBUI_PORT: 8082
      SPARK_PUBLIC_DNS: localhost
      SPARK_WORKER_DIR: /tmp
    links:
      - master
    expose:
      - 7012
      - 7013
      - 7014
      - 7015
      - 7016
      - 8881
    ports:
      - "8082:8082"
      - "4040"
    volumes:
      - ./volumes/spark/conf/worker:/conf
      - ./volumes/spark/data:/tmp/data
      - ./volumes/spark/dependencies:/tmp/dependencies

    depends_on:
      - master

  worker3:
    image: rafabsb/docker-spark-hdfs
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    env_file:
      - ./project.env
    command: /usr/hadoop-2.7.3/etc/hadoop/run.sh
    hostname: worker3
    networks:
      - default_apps
    environment:
      HADOOP_TYPE: datanode
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 4
      SPARK_WORKER_MEMORY: 4g
      SPARK_WORKER_PORT: 8883
      SPARK_WORKER_WEBUI_PORT: 8083
      SPARK_PUBLIC_DNS: localhost
      SPARK_WORKER_DIR: /tmp
    links:
      - master
    expose:
      - 7012
      - 7013
      - 7014
      - 7015
      - 7016
      - 8881
    ports:
      - "8083:8083"
      - "4040"
    volumes:
      - ./volumes/spark/conf/worker:/conf
      - ./volumes/spark/data:/tmp/data
      - ./volumes/spark/dependencies:/tmp/dependencies
      - ./volumes/hadoop/conf:/conf/hadoop
#      - ./volumes/hadoop/datanode3:/data/hadoop_store/hdfs/datanode
    depends_on:
      - master

networks:
  default_apps:
    external: true
